{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c42b15c3-8172-4dc9-afeb-67adc7df5587",
   "metadata": {},
   "source": [
    "# Compare Classifier Package Tutorial\n",
    "A short tutorial on comparing performances of multiple classifiers with Voting or Stacking ensemble methods. **Please note this package only works within the world of scikit-learn.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c565464b-60bd-41ae-9a51-806d694d19f6",
   "metadata": {},
   "source": [
    "## The Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f6d2a1-a68c-4650-b6fa-d7cc70e4106b",
   "metadata": {},
   "source": [
    "### An Example Dataset\n",
    "\n",
    "Here is an example dataset we are getting started with:\n",
    "\n",
    "\n",
    "\n",
    "![example_dataset](./img/dataset.png \"Example Dataset\")\n",
    "\n",
    "This dataset contains physicochemical features as numerical values of wines, along with wine color (column `is_red`: 1 = red; 0 = white), and a target variable, column `quality` representing wine quality score. We are training models to predict wine quality scores, integer values from 3 to 9 (9 = highest quality; 3 = lowest quality).\n",
    "\n",
    "We recommend following the order demonstrated in this tutorial to execute package functions for informed decision making when choosing a final model for your classification task (namely, checking confusion matrices and f1 scores for existing models, then compare f1 scores using ensembles)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e3f8b6-2791-42ef-9bc5-7facf6fa3016",
   "metadata": {},
   "source": [
    "### Step 1: Train + Test Data Split\n",
    "\n",
    "First, you will need to split the dataset into X_train, y_train, X_test, y_test based on features and target variable columns. Here are some [instructions on data splitting](https://www.geeksforgeeks.org/how-to-split-the-dataset-with-scikit-learns-train_test_split-function/), if you need them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7b5da8-deac-41ce-93c4-9d1396d5a8cc",
   "metadata": {},
   "source": [
    "### Step 2: Prepare Your Models\n",
    "\n",
    "Then, you will need to build your scikit-learn machine learning models and perform [hyperparameter tuning](https://scikit-learn.org/1.5/modules/grid_search.html) to make sure they are doing the best they can. We will assume your final candidates as below:\n",
    "\n",
    "```python\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "logreg = LogisticRegression(multi_class='multinomial', solver='lbfgs', C=92)\n",
    "gb = GradientBoostingClassifier(learning_rate=0.001, n_estimators=1000)\n",
    "svm = SVC(kernel='rbf', decision_function_shape='ovr', max_iter=2000)\n",
    "rf = RandomForestClassifier(n_estimators=10)\n",
    "knn5 = KNeighborsClassifier(n_neighbors=5)\n",
    "```\n",
    "\n",
    "Our package functions accept the models in a list of (name, model) tuple, so we will need to convert them into something like this:\n",
    "```python\n",
    "multi_ind = [\n",
    "    ('logreg', logreg),\n",
    "    ('gb', gb),\n",
    "    ('svm', svm),\n",
    "    ('rf', rf),\n",
    "    ('knn5', knn5)\n",
    "]\n",
    "```\n",
    "\n",
    "Note that the names you gave the models will appear in function outputs as labels, so if you'd like to see something more elaborate, for example, `Logistic Regression (C=92)` instead of `logreg` as we have above, please rename your models here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029bc434-47e9-4875-9be4-8123445c281b",
   "metadata": {},
   "source": [
    "> _**A Note on Pipelines:**_ For simplicity's sake, all the models used in this tutorial are individual sklearn classifiers. However, sklearn pipelines are also accepted by our functions, so below is valid input models as well.\n",
    "\n",
    "```python\n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    from sklearn.pipeline import make_pipeline\n",
    "\n",
    "    pipe_svm = make_pipeline(RobustScaler(), svm)\n",
    "    pipe_rf = make_pipeline(RobustScaler(), rf)\n",
    "    pipe_knn5 = make_pipeline(RobustScaler(), knn5)\n",
    "    pipe_gb = make_pipeline(RobustScaler(), gb)\n",
    "    pipe_mnp = make_pipeline(RobustScaler(), mnp)\n",
    "\n",
    "    multi_pipe = [\n",
    "        ('pipe_svm', pipe_svm),\n",
    "        ('pipe_rf', pipe_rf),\n",
    "        ('pipe_knn5', pipe_knn5),\n",
    "        ('pipe_gb', pipe_gb),\n",
    "        ('pipe_mnp', pipe_mnp)\n",
    "    ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61404832-5cb3-4c37-b8a3-9488f0e137c5",
   "metadata": {},
   "source": [
    "## Let's Get Started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca146883-2e33-4af1-993e-bec25793f149",
   "metadata": {},
   "source": [
    "### Step 1: Import Package Functions\n",
    "\n",
    "First, let's import the package functions.\n",
    "\n",
    "```python\n",
    "from compare_classifiers.confusion_matrices import confusion_matrices\n",
    "from compare_classifiers.compare_f1 import compare_f1\n",
    "from compare_classifiers.ensemble_compare_f1 import ensemble_compare_f1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f673bf1-6d86-42a1-b6c4-a5f146601eba",
   "metadata": {},
   "source": [
    "### Step 2: Compare Confusion Matrices for Candidate Models\n",
    "\n",
    "A [confusion matrix](https://www.geeksforgeeks.org/confusion-matrix-machine-learning/) is used to evaluate the performance of a classification model by comparing its predicted values against the actual values, providing a detailed breakdown of correct and incorrect predictions, allowing for a deeper analysis of the model's strengths and weaknesses beyond just overall accuracy. We can take the models we've built in `multi_ind` from the **Prerequisites - Step 2: Prepare Your Models** section and look at how each is performing by looking at their confusion matrices side-by-side to compare them.\n",
    "\n",
    "```python\n",
    "confusion_matrices(multi_ind, X_train, X_test, y_train, y_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46e03ca-15f7-46d8-ba59-0a94ff2a7ff3",
   "metadata": {},
   "source": [
    "You will see outputs like below:\n",
    "\n",
    "![cm1](./img/cm1.png \"cm1\")\n",
    "![cm2](./img/cm2.png \"cm2\")\n",
    "![cm3](./img/cm1.png \"cm3\")\n",
    "![cm4](./img/cm1.png \"cm4\")\n",
    "![cm5](./img/cm5.png \"cm5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703e9852-112e-42b4-b09d-d77d2128bc57",
   "metadata": {},
   "source": [
    "### Step 3: Compare Fit Time and F1 Scores for Candidate Models\n",
    "\n",
    "[F1 score](https://www.v7labs.com/blog/f1-score-guide) is a machine learning metric that measures a model's accuracy by combining its precision and recall by measuring the harmonic mean and is commonly used as an evaluation metric in binary and multi-class classification. Our package uses a macro-averaged F1 score (or macro F1 score). It is computed by taking the arithmetic mean (aka unweighted mean) of all the per-class F1 scores. This method treats all classes equally regardless of their support values.\n",
    "\n",
    "Let's take a look at the fit times and f1 scores using 5-fold cross validation.\n",
    "\n",
    "```python\n",
    "compare_f1(multi_ind, X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0c0af4-5a05-4f9a-ab97-fb2971e9de7c",
   "metadata": {},
   "source": [
    "You will see outputs like below:\n",
    "\n",
    "![f1](./img/f1.png \"f1\")\n",
    "\n",
    "_(Please excuse the low test scores on the test dataset! Due to limited time of the project, I have only trained the models with the first 50 rows of the data using random hyperparamters, so I was not expecting impressing results.)_ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5471bf-8b6d-4316-9dcf-3616334c1959",
   "metadata": {},
   "source": [
    "### Step 4: Compare Fit Time and F1 Scores Through Voting and Stacking Ensembles\n",
    "\n",
    "The goal of sklearn [ensemble methods](https://medium.com/@abhishekjainindore24/different-types-of-ensemble-techniques-bagging-boosting-stacking-voting-blending-b04355a03c93) is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator. There are different types of combination methods to build ensembles and each one yields slightly different results. Our package has two ensembles available from sklearn: [Voting](https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.VotingClassifier.html) and [Stacking](https://scikit-learn.org/dev/modules/generated/sklearn.ensemble.StackingClassifier.html) with default settings. \n",
    "\n",
    "Assume we've chosen the following models: `logreg`, `svm` and `knn5` as our finalists as they show the least amounts of overfitting. We can then take a look at their performances with both ensemble methods, again using 5-fold cross validation.\n",
    "\n",
    "```python\n",
    "finalists = [\n",
    "    ('logreg', logreg),\n",
    "    ('svm', svm),\n",
    "    ('knn5', knn5)\n",
    "]\n",
    "\n",
    "ensemble_compare_f1(finalists, X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034ba086-85ea-4f6a-8b5e-27a2b3d6d2d5",
   "metadata": {},
   "source": [
    "You will see outputs like below:\n",
    "\n",
    "![ensemble_diff](./img/ensemble_diff.png \"ensemble_diff\")\n",
    "\n",
    "As we can see, Voting is clearly the winner!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1aacf0-a45a-43a0-aceb-7a8e7a40553d",
   "metadata": {},
   "source": [
    "### Step 5: Compare Ensemble Results With Individual Classifier Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6ae6d3-bbf8-4cc8-9cd6-eda6abb67dac",
   "metadata": {},
   "source": [
    "This step does not require running any package functions, but we believe it is an essential step in the comparing model performances. Since the Voting f1 score does not show a significant increase from some of our classifiers' individual f1 scores (looking at results from Step 3 above), we might want to assess whether we should choose to go forward with an ensemble model or without.\n",
    "\n",
    "**Now, if you have decided to go with one of the existing individual classifiers you built (let's say `logreg` in our example, since its test score, 0.21, is higher than the Voting ensemble, 0.19, and does not show much sign of overfitting), then you have already ended up with the choice of a best model!**\n",
    "\n",
    "However, if you have decided to use an ensemble to predict instead, then in the next section, we will demonstrate how to use the Voting ensemble method to predict the target classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd23a69-ccfa-47b4-8444-c00e3de14fc2",
   "metadata": {},
   "source": [
    "## (Optional) Predict Using Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d567b14-547c-481d-9e51-d293e185b549",
   "metadata": {},
   "source": [
    "### Predict Unseen Data With Chosen Ensemble Method\n",
    "\n",
    "Now it's time to put our winner ensemble method to test and have it predict some test or unseen data.\n",
    "\n",
    "\n",
    "```python\n",
    "from compare_classifiers.ensemble_predict import ensemble_predict\n",
    "\n",
    "ensemble_predict(finalists, X_train, y_train, 'voting', unseen_data)\n",
    "```\n",
    "\n",
    "> _**Note:**_ Since Voting won in our example, we put in 'voting' as the fourth input parameter of the function. Here, if you would like to use Stacking as the ensemble method, you would say 'stacking' instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272c718f-e695-4805-a752-d20a428e72bc",
   "metadata": {},
   "source": [
    "You will see outputs like below:\n",
    "\n",
    "![predict](./img/predict.png \"predict\")\n",
    "\n",
    "These are the wine quality scores we get on the unseen data with our Voting ensemble classifier from the three finalist models we defined in Step 4!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:compare_classifiers]",
   "language": "python",
   "name": "conda-env-compare_classifiers-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
